# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tVdAHOfz1GZ9fj7-3SY1vpKEAjqNSSkg
"""

from google.colab import files
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# import scipy.io

from sklearn import decomposition
from sklearn import datasets

# data=files.upload()

from google.colab import drive
drive.mount('/content/drive')

src = pd.read_csv("/content/drive/MyDrive/TCGA_LUAD_GENESUMS.csv")

x = src.transpose().drop('gene_id')
num_samples,num_bands=x.shape
# x = x.transpose()
x.loc[~(x==0).all(axis=1)]
print(f"Loaded dataset with {num_samples} samples and {num_bands} bands")
x

gth = pd.read_csv("/content/drive/MyDrive/Patient_Data_cleanpass1.csv")
# gth

y = gth.set_index('external_id')['cgc_sample_sample_type']
y.replace({'Primary Tumor': 1}, inplace=True)
y.replace({'Recurrent Tumor': 0}, inplace=True)
y.replace({'Solid Tissue Normal': 0}, inplace=True)

allData = x.join(y)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
# Load data without headers
data = allData
# Assume the last column is the target variable and the rest are features
X = data.iloc[:, :-1]
y = data.iloc[:, -1]
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build a predictive model (Random Forest Classifier in this example)
model = RandomForestClassifier()
model.fit(X_train, y_train)
# Make predictions on the test set
predictions = model.predict(X_test)
# Evaluate the model
accuracy = accuracy_score(y_test, predictions)
print(f'Model Accuracy: {accuracy}')

#from _typeshed import SupportsDivMod
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier
# Load data without headers
data = allData


# Assume the last column is the target variable and the rest are features
X = data.iloc[:, 1:-1]
y = data.iloc[:, -1]

#print(X)
# Build a Gaussian Naive Bayes model
#model.fit(X_train, y_train)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build a Gaussian Naive Bayes model
model = SGDClassifier()

# Lists to store training progress
accuracy_history = []

# Train the model and track accuracy over iterations
for iteration in range(1):  # Adjust the number of iterations as needed
    model.partial_fit(X_train, y_train, classes=[0, 1])

    # Make predictions on the test set
    predictions = model.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, predictions)
    accuracy_history.append(accuracy)

    print(f'Iteration {iteration + 1}, Model Accuracy: {accuracy}')

# Plot the training progress
plt.plot(range(1, len(accuracy_history) + 1), accuracy_history, marker='o')
plt.title('Training Progress')
plt.xlabel('Iteration')
plt.ylabel('Accuracy')
plt.show()

from google.colab import drive
drive.mount('/content/drive')

#from _typeshed import SupportsDivMod
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.linear_model import SGDClassifier

df = pd.read_csv("/content/drive/MyDrive/generatedDataGAN1000EpochsNormalizedNoActivation.csv")

# Separate features (X) and target variable (y)
X = df.iloc[:, :-1]  # All columns except the last one
y = df.iloc[:, -1]   # Last column

# Separate features (X_val) and target variable (y_val) Secure
X_val = data.iloc[:, :-2]  # All columns except the last one
y_val = data.iloc[:, -1]   # Last column

X_val.shape

# Standardize features by removing the mean and scaling to unit variance
scaler = StandardScaler(with_mean=False)
X_train = scaler.fit_transform(X_train)
X_val = scaler.fit_transform(X_val)

# Estimator implements regularized linear models with stochastic gradient descent (SGD) learning
model = SGDClassifier()

# Lists to store training progress0
accuracy_history = []
val_accuracies = []
# Train the model and track accuracy over iterations
for iteration in range(20):  # Adjust the number of iterations as needed
    model.partial_fit(X_train, y_train, classes=[0, 1])

    # Make predictions on the test set
    predictions = model.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, predictions)
    accuracy_history.append(accuracy)


    # Make predictions on the test set
    y_val_pred = model.predict(X_val)

    # Evaluate the model
    validations_accuracy = accuracy_score(y_val,y_val_pred)
    val_accuracies.append(validations_accuracy)
   # print(f'Iteration {iteration + 1}, Model Accuracy: {accuracy}')
   # print(f'Iteration {iteration + 1}, Validation Accuracy: {validations_accuracy}')

# Plot the training progress
plt.plot(range(1, len(accuracy_history) + 1), accuracy_history,label = 'Training Accuracy', marker='o')
plt.plot(range(1, len(val_accuracies) + 1), val_accuracies,label = 'Validation Accuracy', marker='o')
plt.legend(["Training", "Validation"])
plt.title('Training Progress')
plt.xlabel('Iteration')
plt.ylabel('Accuracy')
plt.show()